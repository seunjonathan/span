
|||
|--|--|
|**Status**|Draft|
|**Version**| v0.3|
|**Date**|31-Mar-2022|
|**Technical owner**|Sarbjit Sarkaria|
|**Business lead**|Yuri Fedoruk|
|**Ticket**| #975 <br> #1236 <br> #1032 <br> #1041 <br> #1746

[[_TOC_]]


# Introduction
Seaspan tugs are installed with sensors that capture vessel telemetry information. The data includes GPS location, engine speed, course & fuel, for example. The data is then made available through a REST API operated by [Maretron](https://maretron.com/). The data is returned by the API in 5 minute increments, formatted as XML. The nature of the data is such that each reading appears as a separate timestamped row in the response. One of the goals of this ETL solution is to transform the data into a tabular form where all readings for the same time are grouped together into a single row. 

This runbook, describes the as-built solution in Azure that procures and curates this data and should help the reader monitor and maintain the solution.

# Technical Solution
The de-facto approach to modern ETL solutions in Azure adopt Data Factory pipelines. These provide an easy way to build scalable mechanisms that  orchestrate the procurement process. The main components are:

1. An Azure Data Factory pipeline, scheduled to trigger once daily.
1. A Logic App that lands the raw XML data from the REST API.
1. A Function App hosting two functions:
3.1 A function that converts XML to a columnar parquet format
3.2 A function that updates a table store based checkpoint
1. A Storage account to persist the raw (incoming) and processed (parquet) data
1. The REST API itself.

Note: The Logic App is necessary, because the Data Factory REST activity will fail unless the response contains JSON content.

## Sequence Diagram
The diagram below shows the processing that takes place each time the pipeline runs:
![1.png](/.attachments/1-938f91ad-ba9f-42e3-b067-7e3bf614325a.png =500x)

## Data Sources
The following are data sources that the solution depends upon:

|Name|Source|Description|Sample|
|--|--|--|--|
|Vessel list|Table store<p>`cfgBronze`|A JSON formatted list of vessel names. This is the list of all vessels for which telemetry data is to be retrieved.<p>The list is stored at<br> `PropertyName`=`Vessel_List`<br>`PartitionKey`=`MARETRON`<br>`RowKey`=`VESSEL` |`{"vessels":[{"name":"Cavalier"},{"name":"Commander"},{"name":"Comox Crown"},{"name":"Corsair"},{"name":"HaiSea Guardian"},{"name":"Protector"},{"name":"Raider"},{"name":"Rogue"},{"name":"Royal"}]}`|
|Checkpoint|Table store<p>`cfgBronze`|The checkpoint is stored at<br> `PropertyName`=`LastProcessed_UTC`<br>`PartitionKey`=`MARETRON`<br>`RowKey`=`VESSEL`|`2021-12-29T07:51:21.0030666Z`|
|Telemetry|REST API|`https://tcs.maretron.com/get_data.php`<p> The logic app calls this endpoint and stores the response as a blob. The name of the file includes the name of the vessel and a timestamp.|![2.png](/.attachments/2-646fff32-1241-44a3-8af5-790e6dd2b854.png)|


## Data Sinks
The goal of this ETL is to convert the data in each XML file into a tabular (parquet) form. One parquet file per XML file is generated. The output parquet files are considered to be at the `bronze` maturity level.

The following describe the data generated by the ETL:
|Name|Sink|Description|Sample|
|--|--|--|--|
|Raw XML|Blob Store|XML from each vessel is persisted in a timestamped file located in the `landing` container of the storage account at `bronze/maretron`. See [2] for more details about the parameters reported in the XML.|Sample filenames:<br>`Cavalier-1640123650.xml`<br>`Corsair-1640123650.xml`|
|Parquet|Blob store|Telemetry data in parquet format for each vessel located in the `processed` container at `bronze/maretron`|This image shows only a sample of the column names.![3.png](/.attachments/3-4d68caf5-469c-4a7e-b3b7-6e298d2c30bb.png)|


# Deployment

## Terminology
In this document the term _deployment_ refers to the steps required to create an instance of this ETL solution. _Provisioning_ is a part of the deployment process in which resources are configured or populated with content, such as code.

When re-creating this solution in a new target resource group, there are 3 steps to the deployment process. They are:
1. Create the resources
![4.png](/.attachments/4-cc85c584-ac1e-4210-b600-52b9d4077e44.png =150x)<p>Note that no storage account is deployed since the data is to land in the common account `adlsedwprod`.
1. Enable resources to access the `adlsedwprod` storage account. See the _Networking_ section below.
1. Provision the resources. See the _Provisioning_ section below.



## Networking

|Resource|Description|
|--|--|
|ADF|The ADF must add a managed private endpoint:<p>![5.png](/.attachments/5-5e19509a-7688-4518-b8b6-3485d58df45f.png =400x)<p><p> `adlsedwprod` must approve the endpoint<p>![6.png](/.attachments/6-55db2245-0ff9-49a0-966a-f21d3a973291.png =400x)|
|Logic App|`adlsedwprod` must grant `Storage Blob Data Contributor` to the managed identity|
|Function App|The function app and `adlsedwprod` must be attached to the same vnet/subnet.<p><p>At `adlsedwprod`:<p>![7.png](/.attachments/7-5c023093-e42d-4f3e-987a-8d4a00bf0b45.png =400x)<p>At the function app:<p>![8.png](/.attachments/8-63918d00-2ec8-4617-ae68-3a3ec3de42b6.png =400x)|

## Provisioning
Currently no automated CI/CD pipeline exists for provisioning the content of the resources. Instead the provisioning is to be done manually. 

|Component|Repo|Description|
|--|--|--|
|ADF|`https://dev.azure.com/seaspan-edw/DataOps/_git/maretron-adf`|Rather than attaching the production ADF to the git repo, the recommendation is to export the ARM template from the desired branch and then import it into this production ADF.  This prevents the possibility of accidently changing contents of a branch.|
|Function App|`https://dev.azure.com/seaspan-edw/DataOps/_git/maretron-functionapp` |The project should be opened in Visual Studio Code and the desired branch selected. This can then be uploaded to the resource. Further details about this function app can be found in the `README.md` file in the git repo.|
|Logic App| NA | At this point, the logic app is not version controlled. The recommendation is to copy the JSON source of the logic app from the `dev` version of the ETL and paste this into the target resource.|


## Environments
This solution exists in two environments/resource groups:
1. `rg-sfc-vesseltelemetry-dev-canadacentral-001`
1. `rg-sfc-vesseltelemetry-prod-canadacentral-002`

The two solutions differ slightly due to difficulties encountered in getting `update-checkpoint` to work in `rg-sfc-vesseltelemetry-dev-canadacentral-001`. The _dev_ version looks like this:
![9.png](/.attachments/9-9cd65f14-bc6f-473c-ba7b-8d03ebeab686.png =350x)<p>This is captured as an item of technical debt.

# Monitoring & Troubleshooting
## Function App
The `xml2parquet` function is responsible for the pivot operation that converts timeseries data from a table in which a single column contains many different readings. Into a table where different readings are placed in their own columns. The code that does this is, in places, dependent upon some of the XML attribute names that appear in the raw data. It's possible that a change could cause failure. In this case the function app monitor may help.<p>
![10.png](/.attachments/10-0c017eca-8524-430f-bbc6-ef327395f912.png =400x)<p><p>

It is also possible to watch the function app live, as it runs. This can be very useful when for example, debugging problems. Live metrics are available from the App insights resource.<p>
![11.png](/.attachments/11-5ab33213-25fc-468e-bd2a-1df55321ae7e.png =350x)<p>
Simply invoke the function app and watch the metrics screen for logs to appear.

To set up your own local function app development environment, see [[1]](#References)

## ADF
Both the ADF and the Logic App should be investigated if problems are suspected.
The standard ADF monitor will display the status of each run.<p>
![12.png](/.attachments/12-f5220432-94ee-4669-ab36-fc8bbf5ed5da.png =350x)<p>

## Logic App
However, even if no errors are reported, it is still possible that a failure has occurred. The Logic App Runs history should also be checked.<p>
![13.png](/.attachments/13-2f3cf648-6817-4833-baf9-e71684a7ea91.png =350x)

# Technical Debt
There are a few outstanding (non-urgent) issues still to address:<p>
#1029
#1030
#1033

# Updates
## Data Factory Pipeline
Since the initial release, the ADF pipeline also triggers processing that aggregates and combines the telemetry data with Towworks events. To do this the pipeline triggers a `maretron2silver` Pyspark script that is deployed in a Databricks workspace. 

![14.png](/.attachments/14-c50ce70c-9f2e-453a-81b1-d6baf33827d1.png)

The `PL_Maretron2Silver` pipeline calls the following Pyspark script: [maretron2silver.py](https://dev.azure.com/seaspan-edw/DataOps/_git/maretron-sparkapp)

* For further details, see: <br> #1236 <br> #1041 <br> #1032 <br> #1481


## Function _api2parquet_
To support #1746, a new function was added to the Azure function app resource. The purpose of this function is to implement a new near-realtime mechanism for retrieval of vessel information. The most efficient way to do this was to deploy a single function that:
* Calls the Maretron API
* Processes and pivots the XML
* Curates current data
* Archives older data

### Design & Sequence Diagram
![15.png](/.attachments/15-0b0331f9-7b67-4919-910f-28075322acb8.png =500x)

1. The function is configured to be triggered every 5 minutes by the Azure Function timer framework.
2. The function calls the Maretron API with parameters that contain a list of vessel names and a time window as follows:
    ```
    end_unixtime   = time.time()             # now
    start_unixtime = end_unixtime - (5 * 60) # 5 minutes ago
    ```
3. A response containing XML is returned.
4. The function processes and pivots the response. A parquet file with a timestamped name is written to a `current` folder.
5. Any files in the `current` folder older than a configurable retention period are moved into an archive folder.

### Configuring Timer

The function is timer triggered using the following CRON based setting in the `function.json` file:
```
{
  "scriptFile": "__init__.py",
  "bindings": [
    {
      "name": "mytimer",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 */5 * * * *"
    }
  ]
}
```
For details on how to modify the schedule see [cron expression](https://en.wikipedia.org/wiki/Cron#CRON_expression).

### Environment Variables
New environment variables must be set as follows:
Variable | Description
-----------|-------------
`BASE_FOLDER`|Target curation folder, e.g. `gold/telem`. Must start with `gold`.
`API_USERNAME`| Maretron API account username
`API_PASSWORD`| Maretron API account password
`API_BASE_URL`| Maretron API base url, e.g. `https://tcs.maretron.com/get_data.php`
`RETENTION_PERIOD_SECONDS`| Retention period, in seconds after which curated files should be archived and moved out of the `BASE_FOLDER`. (They will be moved to `silver/telem`).
`VESSEL_LIST`|Comma separated list of vessel names to be queried. E.g. `Corsair,Cavalier,Comox Crown,Commander,Royal,Rogue,Raider,Protector`

#### Key Vault Access
In order to configure the `API_PASSWORD` variable to refer to a secret in a key vault, e.g. 
  
    @Microsoft.KeyVault(VaultName=akv-edw-dev;SecretName=maretron)

either :
1. The key vault resource must be connected to the virtual network of the function app

![16.png](/.attachments/16-751b5f68-6cdb-4f60-a60d-ed082860b1fa.png)
or
2. The key vault must not be network restricted.

## Status Update
* It was noted on 30th March 2023 that the `Maretron2Silver` PySpark activity in the data factory pipeline was failing due to an `Out of memory` error.
The last successful run was 17th March 2023.
For now the ADF schedule based trigger has been turned off. Until such time that an actual demand for this service appears.




# References
[1] [README.md](https://dev.azure.com/seaspan-edw/DataOps/_git/maretron-functionapp?path=/README.md&version=GBmain&_a=preview`)
[2] Maretron telemetry units:
![17.png](/.attachments/17-161c8899-9502-41bb-9382-3f04d4f47246.png =300x)


