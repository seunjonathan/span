'''
    This file can be uploaded to the Databricks workspace and invoked at the command line
    as a python script.
'''

import argparse
import json
import logging
import os
import pyspark

from delta import configure_spark_with_delta_pip
from delta.tables import DeltaTable
# from dotenv import load_dotenv
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col, desc, row_number
from pyspark.sql.window import Window


log = logging.getLogger(__name__)

# Default logging level is WARNING
#   To see more output, change to logging.INFO
logging.basicConfig(level=logging.WARNING)

# Construct argument parser
ap = argparse.ArgumentParser()
ap.add_argument('-t', '--source_table_name', required=True, help='the name of incoming TOPS table, e.g. EVENTS')
ap.add_argument('-s', '--source_level', required=True, help='path to source folder, e.g. bronze')
ap.add_argument('-d', '--destination_level', required=True, help='path to target folder, e.g. silver')
ap.add_argument('-c', '--schema', required=True, help='schema, e.g. TOPSv2. A folder with this name must exist in the storage account')
ap.add_argument('-x', '--flatten_instructions', required=True, help='a JSON string the specifies instructions for flattening the raw data')
ap.add_argument('-f', '--field_types', required=False, help='a JSON string that enumerates target types of any fields that need to be cast from string', default = None)
ap.add_argument('-i', '--incremental', required=True, help='indicates with "y" or "n" whether the incoming data is to be incrementally added or is a full overwrite')
ap.add_argument('-k', '--pkey', required=False, help='for incrementally processed tables, the column name of primary key in source data')
ap.add_argument('-w', '--watermark', required=False, nargs='?', default='None', help='for incrementally processed tables, the column name to be used as a checkpoint or watermark')

STORAGE_ACCOUNT_NAME = os.getenv("STORAGE_ACCOUNT_NAME")
SPCLIENTID = os.getenv("SPCLIENTID")
SPTENANTID = os.getenv("SPTENANTID")
SPSCOPE = os.getenv("SPSCOPE")
SPSECRET = os.getenv("SPSECRET")

# Constants
BASE_STORAGE_URL_LANDING = f'abfss://landing@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net'
BASE_STORAGE_URL_PROCESSED = f'abfss://processed@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net'


def process_command_line (spark):
    '''
        This method is a wrapper that fetches the arguments from the command line,
        gets any input data frames from storage and calls the main code that processes
        the data.

        This method accepts flattening instructions as a JSON string
        representing a list of JSON objects. The expected structure is as follows:
        [
            {
                "destination_tablename": "EVENTS",
                "flatten_command": "select col.Event.* from (select explode(TOWWORKSEVENTS) from json)"
            }
        ]
        This approach allows a single incoming dataset to generate *multiple* target tables. For example, this is
        a requirement for the TOWWORKS TRANSACTIONS table which creates both TRANSACTIONS table data and
        GLCODES table data.
    '''
    log.info('+++ process_data_wrapper running in spark %s', spark.version)
    args = vars(ap.parse_args())
    p_source_tablename : str = args['source_table_name']
    p_source_level: str = args['source_level']
    p_destination_level: str = args['destination_level']
    p_schema: str = args['schema']
    p_flatten_instructions: str = args['flatten_instructions']
    p_field_types: str = json.loads(args['field_types']) if args['field_types'] is not None else None
    p_incremental: bool = args['incremental'] == 'y' or args['incremental'] == 'Y'
    p_key: str = args['pkey']
    p_watermark: str = args['watermark']

    log.info('+++ p_source_tablename=%s', p_source_tablename)
    log.info('+++ p_source_level=%s', p_source_level)
    log.info('+++ p_destination_level=%s', p_destination_level)
    log.info('+++ p_schema=%s', p_schema)
    log.info('+++ p_flatten_instructions=%s', p_flatten_instructions)
    log.info('+++ p_field_types=%s', p_field_types)
    log.info('+++ p_incremental=%s', p_incremental)
    log.info('+++ p_key=%s', p_key)
    log.info('+++ p_watermark=%s', p_watermark)

    source_table_path = get_path_to_table(spark, BASE_STORAGE_URL_LANDING, p_source_tablename, p_source_level, p_schema)
    # DROPMALFORMED used to address bug 2126 by dropping the '_corrupt_record' column generated by empty JSON files.
    bronze_df = spark.read.option('multiline', True).option('mode','DROPMALFORMED').json(source_table_path)

    instructions = json.loads(p_flatten_instructions)
    for instruction in instructions:
        flatten_command = instruction['flatten_command']
        destination_tablename = instruction['destination_tablename']
        process_instruction(spark, bronze_df, flatten_command, p_field_types, destination_tablename, p_destination_level, p_schema, p_incremental, p_key, p_watermark)


def process_instruction(spark, bronze_df, flatten_command, field_types, destination_tablename, destination_level, schema, is_incremental, key, watermark):
    '''
        The flattened dataframe is persisted to the specified destination level/schema/table, using the
        supplied flatten_command (which is a SQL statement)
    '''
    log.info('+++ Generating data for %s by flattening JSON using %s', destination_tablename, flatten_command)
    silver_df = flatten_and_cast_data(spark, bronze_df, flatten_command, field_types)
    if silver_df is None:
        log.warning('--* source JSON was empty processing %s', destination_tablename)
    else:
        num_rows = silver_df.count()
        log.info('+++ new column names are %s', silver_df.columns)

        target_path = get_path_to_table(spark, BASE_STORAGE_URL_PROCESSED, destination_tablename, destination_level, schema)

        # Now apply/persist these updates to the delta table.
        if not DeltaTable.isDeltaTable(spark, target_path):
            log.warning('+++ Creating new table with %s rows @ %s', num_rows, target_path)
            silver_df.write.format('delta').mode('overwrite').save(target_path)
        else:
            if num_rows > 0:
                if is_incremental: # is incremental
                    target_delta_table = DeltaTable.forPath(spark, target_path)
                    unique_silver_df = latest_record_for_each_key(silver_df, key, watermark)
                    merge(unique_silver_df, target_delta_table, destination_tablename, key)
                else: # is a full load
                    log.warning('+++ Overwriting table with %s rows @ %s', num_rows, target_path)
                    silver_df.write.format('delta').option('mergeSchema', True).mode('overwrite').save(target_path)
            else:
                log.warning('+++ Empty df encountered - nothing changed.')


def latest_record_for_each_key(in_df: DataFrame, with_key: str, using_watermark: str) -> DataFrame:
    '''
        When merging data incrementally into a delta table, there can only be one record
        for each primary key. In the case that there are multiple records with
        the same `key` in the incoming dataframe `df`, this method uses a window
        partition to select the first record ordered according to descending
        `watermark`.
    '''
    w = Window.partitionBy(with_key).orderBy(desc(using_watermark))
    latest_record_for_each_key_df = in_df.withColumn('row', row_number().over(w)).filter(col('row')==1).drop('row')
    return latest_record_for_each_key_df


def flatten_and_cast_data (spark, source_df: DataFrame, flatten_command: str, field_types: dict) -> DataFrame:
    '''
        This method does all the real work. The source dataframe is processed and
        the resulting dataframe returned. 
        The source is expected to be raw JSON data. It is flattened.
        The `field_types` dictionary enumerates the types of any non-string fields that need to
        cast.
        The method apply the supplied SQL in the flatten_command argument to raw JSON loaded
        into a dataframe. The name of this dataframe table is `json`
    '''
    flat_with_adjusted_column_types_df = None
    if source_df.count() > 0:
        source_df.createOrReplaceTempView('json')
        flat_df = spark.sql(flatten_command)
        flat_with_adjusted_column_types_df = cast_column_types(flat_df, field_types)

    return flat_with_adjusted_column_types_df


def merge(df: DataFrame, into_target_delta_table: DeltaTable, destination_tablename: str, using_key_column: str):
    '''
        This method will merge the provided DataFrame `df`, `into_target_delta_table` `using_key_column`.
        I.e. if records with the same primary key already exist in the target, the record is updated,
        otherwise it is inserted.
        This method only supports the case when the primary key is a single column.
    '''
    log.warning('+++ Merging incoming %s rows into %s using column %s', df.count(), destination_tablename, using_key_column)

    # Build merge predicate
    try:
        predicate_string = f'target.{using_key_column} = updates.{using_key_column}'

        log.info('+++ Merging with predicate: %s', predicate_string)
        into_target_delta_table.alias('target') \
            .merge(df.alias('updates'), predicate_string) \
            .whenMatchedUpdateAll() \
            .whenNotMatchedInsertAll() \
            .execute()
    except IndexError:
        log.error('+++ Exception merging')


def cast_column_types(df: DataFrame, types: dict) -> DataFrame:
    '''
        This method takes a dataframe and looks for columns that match
        those specified by the `types` argument. Format of this is:
        {
            'datetimes':[<csv of field names>],
            'doubles':[<csv of field names>],
            'ints':[<csv of field names>]
        }
        (csv of field names is case insensitive)
        If they match, the column is cast to Spark timestamp type, double
        or int accordingly.
        TODO: Log bad conversions somewhere/somehow. E.g. bad datetime or double
    '''
    if types is not None:
        date_columns = [f.lower() for f in types['datetimes']]
        double_columns = [f.lower() for f in types['doubles']]
        int_columns = [f.lower() for f in types['ints']]
        columns = df.columns
        for c in columns:
            if c.lower() in date_columns:
                df = df.withColumn(c, col(c).cast('timestamp'))
            elif c.lower() in double_columns:
                df = df.withColumn(c, col(c).cast('double'))
            elif c.lower() in int_columns:
                df = df.withColumn(c, col(c).cast('int'))
    return df


def get_path_to_table(spark, base_url, name_of_table, level, schema) -> str:
    '''
        Given the name of a table, this method will return a path
        to a local storage folder when running in spark local mode.
        Otherwise a path to blob storage will be returned.
    '''
    if spark.conf.get('spark.master') == 'local':
        return os.path.join(os.getcwd(), 'storage', f'{level}', f'{schema}', f'{name_of_table}')
    else:
        return f'{base_url}/{level}/{schema}/{name_of_table}'



def get_or_create_spark(spark_app_name: str):
    '''
        This method is to factor out the same code that is used in a few
        places inside this project.
        Note that in Windows, a new spark context will create a temporary folder.
        These folders will be locate in the /tmp folder of this project.
        A known error is expected that indicates that this temp spark folder
        could not be deleted when the spark context stops. You will have to delete
        these folders in /tmp manually.
    '''
    builder = pyspark.sql.SparkSession.builder.appName(spark_app_name) \
        .config("spark.master", "local") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.local.dir", "./tmp/spark-temp")

    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    return spark

if __name__ == "__main__":
    # If running as main, context must be a real spark job

    spark.conf.set(f"fs.azure.account.auth.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net", "OAuth")
    spark.conf.set(f"fs.azure.account.oauth.provider.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
    spark.conf.set(f"fs.azure.account.oauth2.client.id.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net", SPCLIENTID)
    spark.conf.set(f"fs.azure.account.oauth2.client.secret.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net", dbutils.secrets.get(SPSCOPE,SPSECRET))
    spark.conf.set(f"fs.azure.account.oauth2.client.endpoint.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net", f"https://login.microsoftonline.com/{SPTENANTID}/oauth2/token")
    process_command_line(spark)
